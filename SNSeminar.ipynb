{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9398d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would you like to train Aurora?: Yes\n",
      "You: how are you\n",
      "\n",
      "[Foundation Output]:  how are youI'm good, how are you?\n",
      "[LSTM Output]: tensor([[-0.0207, -0.3179,  0.1043, -0.0201,  0.2208, -0.1027, -0.0597, -0.1454,\n",
      "         -0.3617, -0.0639, -0.2784,  0.0127,  0.0818,  0.0957,  0.3663,  0.3111,\n",
      "          0.3488,  0.5083, -0.1074,  0.1005, -0.1142, -0.2859, -0.3738, -0.4298,\n",
      "         -0.0197, -0.1975]], grad_fn=<AddmmBackward0>)\n",
      "[SNN Spikes]: tensor([0., 1., 1., 1., 1.])\n",
      "Would you like to continue training Aurora?: yes\n",
      "You: ABCDEFG. What comes next?\n",
      "\n",
      "[Foundation Output]:  ABCDEFG. What comes next?ABCDEGF. What comes next?\n",
      "[LSTM Output]: tensor([[-0.0230, -0.3090,  0.0961, -0.0220,  0.2148, -0.0905, -0.0587, -0.1516,\n",
      "         -0.3515, -0.0659, -0.2797,  0.0045,  0.0826,  0.1019,  0.3749,  0.3088,\n",
      "          0.3404,  0.5005, -0.1010,  0.1036, -0.1085, -0.2725, -0.3708, -0.4283,\n",
      "         -0.0177, -0.1934]], grad_fn=<AddmmBackward0>)\n",
      "[SNN Spikes]: tensor([0., 1., 1., 1., 1.])\n",
      "Would you like to continue training Aurora?: No\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import threading\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class Foundation:\n",
    "    def __init__(self):\n",
    "        # Loading DialoGPT-Medium\n",
    "        self.ltm = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "        \n",
    "    def generate_text(self, prompt):\n",
    "        # ENcode the prompt and generate a response\n",
    "        inputs = self.tokenizer.encode(prompt + self.tokenizer.eos_token, return_tensors=\"pt\")\n",
    "        response = self.ltm.generate(inputs, max_length=1000, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        return self.tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "    \n",
    "    def process_input(self, input_data):\n",
    "        # Generate text based on the input\n",
    "        prompt = f\"Memory query: {input_data}\"\n",
    "        return self.generate_text(prompt)\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, stm_capacity=5):\n",
    "        self.stm_capacity = stm_capacity # Limit for short-term memory\n",
    "        self.stm_head = None\n",
    "        self.stm_size = 0\n",
    "        self.ltm = Foundation() # Long-term memory storage\n",
    "        self.mutex = threading.Lock() # Mutex for seamless access\n",
    "        self.adaptive_weights = {} # Adaptive learning storage\n",
    "    \n",
    "\n",
    "class STM_Level_One(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(STM_Level_One, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Initialize hidden state\n",
    "        h_zero = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h_zero)\n",
    "        # Decode the output for the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        lstm_out,_=self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :]) # Get last time-step output\n",
    "        return out\n",
    "    \n",
    "    # Hybrid LSTM + SNN Integration\n",
    "    def hybrid_model_step(model, snn_layer, memory_manager, optimizer, criterion, memory_sequence, num_epochs=10):\n",
    "        # Convert memory to numerical data by assigning each letter to number\n",
    "        letter_to_index = {letter: idx for idx, letter in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "        index_to_letter = {idx: letter for idx, letter in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "        \n",
    "        # Reshape input for LSTM\n",
    "        inputs = torch.Tensor(data).view(1, -1, 1)\n",
    "        targets = torch.tensor([target], dtype=torch.long) # Ensure 1D tensor with integer class index\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "class SNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, num_neurons, threshold=-1.0, decay=0.9):\n",
    "        super(SNNLayer, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.input_size = input_size\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "        self.neuron_potentials = torch.zeros(num_neurons)\n",
    "        self.neuron_spikes = torch.zeros(num_neurons)\n",
    "        \n",
    "        # Synaptic weights (connections between input and neurons)\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(input_size, num_neurons))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # Reshape to match syn_shape\n",
    "        self.neuron_potentials *= self.decay # Decay the neuron potentials over time\n",
    "        self.neuron_potentials += torch.matmul(x, self.synaptic_weights).squeeze(0)\n",
    "        \n",
    "        # Check if neurons fire (spike) based on threshold\n",
    "        self.neuron_spikes = (self.neuron_potentials >= self.threshold).float()\n",
    "        self.neuron_potentials *= (1-self.neuron_spikes)\n",
    "        \n",
    "        return self.neuron_spikes\n",
    "\n",
    "    \n",
    " # Step 2: Initialize Models\n",
    "foundation = Foundation()\n",
    "memory_manager = MemoryManager()\n",
    "snn_layer = SNNLayer(input_size=26, num_neurons=5) # Adjust input_size to match your needs\n",
    "lstm_model = LSTMModel(input_size=1, hidden_size=32, output_size=26)\n",
    "# Optional: Sample STM\n",
    "stm = STM_Level_One(input_size=1, hidden_size=16, num_layers=1, output_size=26)    \n",
    "\n",
    "def neurolink(foundation, memory_manager, snn_layer, lstm_model, stm):\n",
    "    \"\"\"Connect these models together to establish neurolink returns output after passing through all of them\"\"\"\n",
    "    user_input = input(\"You: \")\n",
    "   \n",
    "    # Step 3: Language -> Token\n",
    "    generated_text = foundation.generate_text(user_input)\n",
    "    print(\"\\n[Foundation Output]: \", generated_text)\n",
    "    \n",
    "    # Step 4: Token-level encoding (example: basic character-level numerical encoding)\n",
    "    letter_to_index = {ch: idx for idx, ch in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "    data = [letter_to_index.get(ch.lower(), 0) for ch in generated_text if ch.lower() in letter_to_index]\n",
    "    if len(data) == 0:\n",
    "        return '[Error]: No valid data from input.'\n",
    "    \n",
    "    input_tensor = torch.Tensor(data).view(1, -1, 1)\n",
    "    \n",
    "    # Step 5: Process through LSTM\n",
    "    lstm_output = lstm_model(input_tensor)\n",
    "    print(\"[LSTM Output]:\", lstm_output)\n",
    "    \n",
    "    # Step 6: SNN Layer step\n",
    "    snn_output = snn_layer(lstm_output)\n",
    "    print(\"[SNN Spikes]:\", snn_output)\n",
    "    \n",
    "    return snn_output\n",
    "\n",
    "start_input = input(\"Would you like to train Aurora?: \")\n",
    "\n",
    "while start_input.lower() in ['yes', 'y', 'yeah', 'sure']:\n",
    "    neurolink(foundation, memory_manager, snn_layer, lstm_model, stm)\n",
    "    start_input = input(\"Would you like to continue training Aurora?: \")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67c712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f97a13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): i\n",
      "Enter a lowercase letter a-i (or 'q' to quit): iii\n",
      "Invalid input. Try a letter between a and i.\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): b\n",
      "Enter a lowercase letter a-i (or 'q' to quit): c\n",
      "Enter a lowercase letter a-i (or 'q' to quit): i\n",
      "Enter a lowercase letter a-i (or 'q' to quit): b\n",
      "Enter a lowercase letter a-i (or 'q' to quit): i\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): b\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): i\n",
      "Enter a lowercase letter a-i (or 'q' to quit): c\n",
      "Enter a lowercase letter a-i (or 'q' to quit): f\n",
      "Enter a lowercase letter a-i (or 'q' to quit): f\n",
      "Enter a lowercase letter a-i (or 'q' to quit): f\n",
      "Enter a lowercase letter a-i (or 'q' to quit): g\n",
      "Enter a lowercase letter a-i (or 'q' to quit): i\n",
      "Enter a lowercase letter a-i (or 'q' to quit): w\n",
      "Invalid input. Try a letter between a and i.\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): \n",
      "Invalid input. Try a letter between a and i.\n",
      "Enter a lowercase letter a-i (or 'q' to quit): g\n",
      "Enter a lowercase letter a-i (or 'q' to quit): f\n",
      "Enter a lowercase letter a-i (or 'q' to quit): f\n",
      "Enter a lowercase letter a-i (or 'q' to quit): a\n",
      "Enter a lowercase letter a-i (or 'q' to quit): q\n",
      "Invalid input. Try a letter between a and i.\n",
      "\n",
      "--- Learning Summary ---\n",
      "Most reinforced info: f\n",
      "Weight: 11849\n",
      "All weights: {'a': 1410, 'b': 525, 'c': 824, 'd': 0, 'e': 0, 'f': 11849, 'g': 5917, 'h': 0, 'i': 6926}\n"
     ]
    }
   ],
   "source": [
    "information = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6, 'g':7, 'h':8, 'i':9}\n",
    "\n",
    "# Start weights and learninh counters\n",
    "weights = {key: 0 for key in information}\n",
    "learning_log = []\n",
    "\n",
    "weight_limit = 100\n",
    "x = 0\n",
    "\n",
    "while x < weight_limit:\n",
    "    user_input = input(\"Enter a lowercase letter a-i (or 'q' to quit): \")\n",
    "    if user_input == 'q':\n",
    "        x = 101\n",
    "        \n",
    "    if user_input in information:\n",
    "        value = information[user_input]\n",
    "        x += 1\n",
    "        \n",
    "        # Update weight based on learning rule (can be expanded)\n",
    "        weights[user_input] += (value * x**2 + 1)\n",
    "        learning_log.append(user_input)\n",
    "    \n",
    "    else:\n",
    "        print(\"Invalid input. Try a letter between a and i.\")\n",
    "        \n",
    "#Output the most learned information\n",
    "most_learned = max(weights, key=weights.get)\n",
    "print(\"\\n--- Learning Summary ---\")\n",
    "print(\"Most reinforced info:\", most_learned)\n",
    "print(\"Weight:\", weights[most_learned])\n",
    "print(\"All weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "429304da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type something ('q' to quit): a\n",
      "Type something ('q' to quit): b\n",
      "Type something ('q' to quit): c\n",
      "Type something ('q' to quit): d\n",
      "Type something ('q' to quit): e\n",
      "Input shape:  torch.Size([5, 1])\n",
      "Max token ID:  68  | Vocab size: 50257\n",
      "Min token ID: 64\n",
      "Logits shape: torch.Size([5, 1, 100])\n",
      "Targets shape: torch.Size([5])\n",
      "Target values: tensor([67, 64, 66, 68, 65])\n",
      "torch.Size([5, 64])\n",
      "torch.Size([64, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (50) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 400\u001b[0m\n\u001b[1;32m    397\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Run real-time training\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m real_time_training(model, snn_layer, memory_manager, optimizer, criterion)\n",
      "Cell \u001b[0;32mIn[50], line 380\u001b[0m, in \u001b[0;36mreal_time_training\u001b[0;34m(model, snn_layer, memory_manager, optimizer, criterion, max_inputs, batch_size)\u001b[0m\n\u001b[1;32m    377\u001b[0m memory_manager\u001b[38;5;241m.\u001b[39madd_memory_unit(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, entropy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Simulate SNN layer with the LSTM output\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m spikes \u001b[38;5;241m=\u001b[39m snn_layer(lstm_output)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Update the memory manager after SNN simulation\u001b[39;00m\n\u001b[1;32m    383\u001b[0m memory_manager\u001b[38;5;241m.\u001b[39mcognitive_score()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSCI380/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CSCI380/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[50], line 200\u001b[0m, in \u001b[0;36mSNNLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynaptic_weights\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Print the weights shape\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_potentials \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay \u001b[38;5;66;03m# Decay the neuron potentials over time\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_potentials \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msynaptic_weights)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Check if neurons fire (spike) based on threshold\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_spikes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuron_potentials \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (50) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#---Configuration---\n",
    "#tokens = [token for token in tokens if token < tokenizer.vocab_size]\n",
    "vocab_size = tokenizer.vocab_size # Adjust based on your tokenized dataset\n",
    "embedding_dim = 64\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "block_size = 32 # Sequence length\n",
    "ffn_hidden = 128\n",
    "#input = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "\n",
    "#--- Simple Transformer Block ---\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ffn_hidden):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embedding_dim, num_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(embedding_dim)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, ffn_hidden),\n",
    "                                nn.ReLU(), nn.Linear(ffn_hidden, embedding_dim),)\n",
    "        self.ln2 = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output,_ = self.attn(x,x,x)\n",
    "        x = self.ln1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.ln2(x + ffn_output)\n",
    "        return x\n",
    "    \n",
    "class MemoryManager:\n",
    "    def __init__(self, decay_factor=0.99, threshold=0.5):\n",
    "        self.decay_factor = decay_factor # Decay factor for memory weight\n",
    "        self.threshold = threshold # Cognitive score threshold for LTM promotion\n",
    "        self.memory_units = [] # Store memory units (information)\n",
    "        \n",
    "    def update_weight(self, w_i):\n",
    "        \"\"\"Apply decay to the memory weight.\"\"\"\n",
    "        return w_i * self.decay_factor # Apply decay to weight\n",
    "    \n",
    "    def f(self,n):\n",
    "        \"\"\"Fibonacci-like recursive function for familiarity adjustment\"\"\"\n",
    "        if n == 0 or n == 1:\n",
    "            return 1\n",
    "        return self.f(n-1) + self.f(n-2)\n",
    "    \n",
    "    def cognitive_score(self):\n",
    "        \"\"\"Calculate the cognitive score based on the weighted information, entropy, and Fibonacci adjustment.\"\"\"\n",
    "        \n",
    "        score = 0\n",
    "        for unit in self.memory_units:\n",
    "            weight = self.update_weight(unit['weight'])\n",
    "            entropy = unit['entropy']\n",
    "            fibonacci = self.f(unit['N-1']) + self.f(unit['N-2']) # Recursive function for familiarity\n",
    "            score += (2 ** unit['k']-1) * weight * entropy * fibonacci\n",
    "            \n",
    "        # Normalize by dividing by the number of memory units (or other normalization logic)\n",
    "        score /= len(self.memory_units)\n",
    "        \n",
    "        # Decision: promote to LTM or retain STM\n",
    "        if score >= self.threshold:\n",
    "            return 'Promote to LTM'\n",
    "        else:\n",
    "            return 'Retain in STM'\n",
    "        return score\n",
    "    \n",
    "    def add_memory_unit(self, weight, entropy, N, k):\n",
    "        \"\"\"Add a new memory unit to the memory system.\"\"\"\n",
    "        unit = {'weight': weight, 'entropy': entropy, 'N-1': N-1, 'N-2': N-2, 'k':k}\n",
    "        self.memory_units.append(unit)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear all memory units.\"\"\"\n",
    "        self.memory_units = []\n",
    "        \n",
    "# Example usage:\n",
    "\n",
    "# Initialize memory manager\n",
    "#memory_manager = MemoryManager(decay_factor=0.99, threshold=0.5)\n",
    "\n",
    "#add some memory units to simulate information\n",
    "#memory_manager.add_memory_unit(weight=0.8, entropy=0.2, N=3, k=1)\n",
    "#memory_manager.add_memory_unit(weight=0.7, entropy=0.3, N=4, k=2)\n",
    "\n",
    "# Compute the cognitive score and determine memory transfer\n",
    "#status = memory_manager.cognitive_score()\n",
    "#print(status)\n",
    "\n",
    "class STM_Level_One(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(STM_Level_One, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Initialize hidden state\n",
    "        h_zero = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h_zero)\n",
    "        # Decode the output for the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        lstm_out,_=self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :]) # Get last time-step output\n",
    "        return out\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Hybrid LSTM + SNN Integration\n",
    "    def hybrid_model_step(model, snn_layer, memory_manager, optimizer, criterion, memory_sequence, num_epochs=10):\n",
    "        # Iterate over the memory sequence\n",
    "        for epoch in range(num_epochs):\n",
    "            for data in memory_sequence:\n",
    "            # Tokenize the input sequence using the tokenizer\n",
    "                inputs = tokenizer.encode(data, add_special_tokens=True)  # Tokenize text to numerical tokens\n",
    "                # Skip too-short sequences\n",
    "                if len(token_ids) < 2:\n",
    "                    continue\n",
    "                inputs = torch.tensor(inputs, dtype=torch.long).unsqueeze(0)  # Reshape for LSTM input\n",
    "            \n",
    "            # Use the last token as the target\n",
    "                targets = torch.tensor([inputs[0, -1]], dtype=torch.long)  # The last token as the target\n",
    "            \n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # LSTM forward pass\n",
    "                lstm_output = model(inputs)\n",
    "            \n",
    "            # Calculate loss with the target\n",
    "                loss = criterion(lstm_output, targets)\n",
    "            \n",
    "            # Backpropagate loss and optimize LSTM weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                #MemoryManager Interaction\n",
    "                #Let's assume: entropy ~ loss.item(), weight = inverse of loss\n",
    "                weight = max(1.0 - loss.item(), 0.0)\n",
    "                entropy = loss.item()\n",
    "            \n",
    "                # Familiarity indicators (example: token length as a proxy)\n",
    "                N = len(inputs)\n",
    "                k = int(torch.argmax(lstm_output).item()) if lstm_output.ndim > 1 else int(lstm_output.item())\n",
    "            \n",
    "            # Update memory (using the memory manager)\n",
    "                memory_manager.add_memory_unit(weight=weight, entropy=entropy, N=N, k=k)\n",
    "            \n",
    "            # Simulate spiking neural network behavior with SNN layer\n",
    "                spikes = snn_layer(lstm_output)\n",
    "            \n",
    "            # Update the memory manager based on the results of the SNN spikes (if necessary)\n",
    "                memory_manager.cognitive_score()\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "                print(f\"Spikes: {spikes}\")\n",
    "            \n",
    "            # Evaluate memory promotion after each epoch\n",
    "            decision = memory_manager.cognitive_score()\n",
    "            print(f\"Epoch {epoch+1} -> Cognitive Score Decision: {decision}\")\n",
    "\n",
    "        \n",
    "class SNNLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_neurons, threshold=-1.0, decay=0.9):\n",
    "        super(SNNLayer, self).__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.threshold = threshold\n",
    "        self.decay = decay\n",
    "        self.neuron_potentials = torch.zeros(num_neurons)\n",
    "        self.neuron_spikes = torch.zeros(num_neurons)\n",
    "        \n",
    "        # Synaptic weights (connections between input and neurons)\n",
    "        self.synaptic_weights = nn.Parameter(torch.randn(self.input_size, output_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size(1) != self.input_size:\n",
    "            raise ValueError(f\"Input size should be {self.input_size}, but got {x.size(1)}\")\n",
    "            \n",
    "        x = x.view(-1,64) # Reshape to match syn_shape\n",
    "        print(x.shape)  # Print the input shape\n",
    "        print(self.synaptic_weights.shape)  # Print the weights shape\n",
    "\n",
    "        self.neuron_potentials *= self.decay # Decay the neuron potentials over time\n",
    "        self.neuron_potentials += torch.matmul(x, self.synaptic_weights).squeeze(1)\n",
    "        \n",
    "        # Check if neurons fire (spike) based on threshold\n",
    "        self.neuron_spikes = (self.neuron_potentials >= self.threshold).float()\n",
    "        self.neuron_potentials *= (1-self.neuron_spikes)\n",
    "        \n",
    "        return self.neuron_spikes\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "store_memory = []\n",
    "\n",
    "def memory_storer(input_text):\n",
    "    tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "    store_memory.append(tokens)\n",
    "    return store_memory\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text data at idx from the dataset\n",
    "        input_text = self.data[idx]\n",
    "        \n",
    "        # Convert the tokenized input into a Pytorch Tensor\n",
    "        #tensor_input = torch.tensor(tokenized_input)\n",
    "        \n",
    "        return torch.tensor(input_text) # Return the tokenized tensor\n",
    "    \n",
    "# --- Tiny GPT Model ---\n",
    "class AuroraGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, block_size, ffn_hidden):\n",
    "        super().__init__()\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.token_embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embedding_dim, num_heads, ffn_hidden) for _ in range(num_layers)])\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
    "        pos_emb = self.position_embedding(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    \n",
    "class MetricsLogger:\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.entropies = []\n",
    "        self.weights = []\n",
    "        self.cognitive_scores = []\n",
    "        self.ltm_promotions = []\n",
    "    \n",
    "    def log_step(self, loss, output_tensor, manager, N, k):\n",
    "        self.losses.append(loss.item())\n",
    "        \n",
    "        # Simulated entropy and weight proxies\n",
    "        entropy = output_tensor.std().item()\n",
    "        weight = output_tensor.abs().mean().item()\n",
    "        \n",
    "        self.entropies.append(entropy)\n",
    "        self.weights.append(weight)\n",
    "        \n",
    "        # Memory management check\n",
    "        manager.add_memory_unit(weight, entropy, N=N, k=k)\n",
    "        decision = manager.cognitive_score()\n",
    "        self.cognitive_scores.append(manager.cognitive_score)\n",
    "        \n",
    "        if decision == 'Promote to LTM':\n",
    "            self.ltm_promotions.append(0)\n",
    "        else:\n",
    "            self.ltm_promotions.append(0)\n",
    "    \n",
    "    def plot_all(self, model_name=\"AuroraGPT\"):\n",
    "        epochs = list(range(len(self.losses)))\n",
    "        \n",
    "        fig, axs = plt.subplots(2, 2, figsize=(14,10))\n",
    "        fig.suptitle(f\"Training Metrics for {model_name}\", fontsize=16)\n",
    "        \n",
    "        # Loss over epochs\n",
    "        axs[0,0].plot(epochs, self.losses, label=\"Loss\")\n",
    "        axs[0,0].set_title(\"Loss Over Epochs\")\n",
    "        axs[0,0].set_xlabel(\"Epoch\")\n",
    "        axs[0,0].set_ylabel(\"Loss\")\n",
    "        axs[0,0].legend()\n",
    "        \n",
    "        # LTM promotions count\n",
    "        ltm_total = sum(self.ltm_promotions)\n",
    "        axs[0,1].bar([\"Promoted to LTM\", \"Retained in STM\"],[ltm_total, len(self.ltm_promotions) - ltm_total], color=['green', 'orange'])\n",
    "        axs[0,1].set_title(\"Memory Promotion Decisions\")\n",
    "        \n",
    "        # Cognitive score histogram\n",
    "        axs[1,0].hist(self.cognitive_scores, bins=10, color='blue', alpha=0.7)\n",
    "        axs[1,0].set_title(\"Cognitive Score Distribution\")\n",
    "        axs[1,0].set_xlabel(\"Score\")\n",
    "        axs[1,0].set_ylabel(\"Frequency\")\n",
    "        \n",
    "        # Entropy vs. Weight scatter\n",
    "        axs[1,1].scatter(self.entropies, self.weights, alpha=0.5, color='purple')\n",
    "        axs[1,1].set_title(\"Entropy vs. Weight\")\n",
    "        axs[1,1].set_xlabel(\"Entropy (std of output)\")\n",
    "        axs[1,1].set_ylabel(\"Avg. Output Weight\")\n",
    "        \n",
    "        plt.tight_layout(rect=[0,0.03,1,0.95])\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "model = AuroraGPT(vocab_size=tokenizer.vocab_size, embedding_dim=embedding_dim, block_size=block_size, num_layers=num_layers, num_heads=num_heads, ffn_hidden=ffn_hidden)\n",
    "def real_time_training(model, snn_layer, memory_manager, optimizer, criterion, max_inputs=15, batch_size=5):\n",
    "    logger = MetricsLogger() \n",
    "    num_inputs = 0\n",
    "    while num_inputs < max_inputs:\n",
    "        user_input = input(\"Type something ('q' to quit): \")\n",
    "        \n",
    "        # Exit condition if the user types 'q'\n",
    "        if user_input == 'q':\n",
    "            print(\"Training has stopped\")\n",
    "            break\n",
    "        \n",
    "        # Store memory and process input text\n",
    "        memory_storer(user_input)\n",
    "        num_inputs += 1\n",
    "        \n",
    "        # Train after collecting batch_size inputs\n",
    "        if num_inputs % batch_size == 0:\n",
    "            # Prepare batch data for training\n",
    "            dataset = TextDataset(store_memory)\n",
    "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Training loop over the batch\n",
    "            model.train()\n",
    "            for data in dataloader:\n",
    "                inputs = data\n",
    "                inputs = inputs.to(model.device) # Ensure input is on the correct device\n",
    "                \n",
    "                print(\"Input shape: \", inputs.shape)\n",
    "                print(\"Max token ID: \", inputs.max().item(), \" | Vocab size:\", model.vocab_size)\n",
    "                print(\"Min token ID:\", inputs.min().item())\n",
    "                \n",
    "                # Pass through AuroraGPT (tokenization and Transformer)\n",
    "                logits = model(inputs)\n",
    "                \n",
    "                # LSTM step\n",
    "                lstm_output = model(inputs) # could be same model or a seperate LSTM model\n",
    "                \n",
    "                # Calculate loss for the LSTM output\n",
    "                print(\"Logits shape:\", lstm_output.shape)\n",
    "                print(\"Targets shape:\", inputs.squeeze().shape)\n",
    "                lstm_output = lstm_output[:, :, :64] # Take only the first 64 features if LSTM size is too big\n",
    "                lstm_output = lstm_output.view(-1,64)\n",
    "                target = inputs.squeeze()\n",
    "                print(\"Target values:\", target)\n",
    "                target = torch.clamp(target, 0, 63)\n",
    "                loss = criterion(lstm_output, target) # For simplicity, using inputs as targets\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                #Log metrics\n",
    "                logger.log_step(loss, lstm_output, memory_manager, N=num_inputs, k=2)\n",
    "                \n",
    "                # Update memory based on the LSTM output (as a simulated step)\n",
    "                memory_manager.add_memory_unit(weight=0.5, entropy=0.3, N=5, k=2)\n",
    "                \n",
    "                # Simulate SNN layer with the LSTM output\n",
    "                spikes = snn_layer(lstm_output)\n",
    "                \n",
    "                # Update the memory manager after SNN simulation\n",
    "                memory_manager.cognitive_score()\n",
    "                \n",
    "                print(f\"Epoch {num_inputs // batch_size}, Loss: {loss.item()}\")\n",
    "                print(f\"Spikes: {spikes}\")\n",
    "                \n",
    "    print(\"Maximum number of inputs reached or stopped by user. Training complete.\")\n",
    "    logger.plot_all(model_name=\"AuroraGPT Alpha\")\n",
    "    \n",
    "# Initialize all components:\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "memory_manager = MemoryManager(decay_factor=0.99, threshold=0.5)\n",
    "model = AuroraGPT(vocab_size=100, embedding_dim=64, num_heads=2, num_layers=2, block_size=32, ffn_hidden=128)\n",
    "snn_layer = SNNLayer(input_size=embedding_dim, output_size=block_size, num_neurons=50)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Run real-time training\n",
    "real_time_training(model, snn_layer, memory_manager, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b78abaa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AuroraGPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ---Example usage---\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AuroraGPT(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m      3\u001b[0m                  embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m      4\u001b[0m                  num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[1;32m      5\u001b[0m                  num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m      6\u001b[0m                  block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[1;32m      7\u001b[0m                  ffn_hidden\u001b[38;5;241m=\u001b[39mffn_hidden)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Random input for testing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (\u001b[38;5;241m2\u001b[39m, block_size))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AuroraGPT' is not defined"
     ]
    }
   ],
   "source": [
    "# ---Example usage---\n",
    "model = AuroraGPT(vocab_size=vocab_size,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 num_heads=num_heads,\n",
    "                 num_layers=num_layers,\n",
    "                 block_size=block_size,\n",
    "                 ffn_hidden=ffn_hidden)\n",
    "\n",
    "# Random input for testing\n",
    "input_ids = torch.randint(0, vocab_size, (2, block_size))\n",
    "logits = model(input_ids)\n",
    "print(logits.shape) # Should be (batch, sequence, vocab_size\n",
    "\n",
    "x = 100\n",
    "    \n",
    "while x < 101:\n",
    "    user_input = input(\"Type something ('q to quit'): \")\n",
    "    memory_storer(user_input)\n",
    "    \n",
    "    if len(user_input) >= 25:\n",
    "        print(\"Stored tokens: \", store_memory)\n",
    "        x = 102\n",
    "    \n",
    "    elif user_input == 'q':\n",
    "        print(\"Stored tokens: \", store_memory)\n",
    "        x = 102\n",
    "        \n",
    "dataset = TextDataset(store_memory)\n",
    "\n",
    "# Example: Iterate through the dataset to get the tokenized tensor data\n",
    "for idx in range(len(dataset)):\n",
    "    print(f\"Sample{idx}: {dataset[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4cac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
